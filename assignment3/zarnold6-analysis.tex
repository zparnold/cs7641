%! Author = zarnold
%! Date = 9/27/20

% Preamble
\documentclass[11pt]{article}
\usepackage[letterpaper]{geometry}
\usepackage{cite}
\usepackage{url}
\usepackage{fancyhdr}
% Packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{wrapfig}
\usepackage[font=small,labelfont=bf]{caption}

\fancypagestyle{firstpage}
{
\fancyhead[L]{}
\fancyhead[R]{Zach Arnold \linebreak CS 7641 \linebreak Assignment 3}
\setlength{\headheight}{52pt}
}
\graphicspath{./}
\newcommand{\datasetone}{Census data to predict salary band}
\newcommand{\datasettwo}{Banking data to predict response}
% Document
\begin{document}
    \thispagestyle{firstpage}


    \section{Introduction}\label{sec:introduction}
    This paper will explore several unsupervised learning and dimensionality reduction techniques on the datasets I've
    used for earlier assignments.
    It begins with a discussion of the datasets, continues with exploration of clustering, then analyzes various dimensionality
    reduction techniques on neural network performance and finally ends with a combination of clustering and dimensionality
    reduction as it pertains to NN performance.
    Unless otherwise stated, scikit-learn's libraries were used to generate models, results, and graphs.


    \section{Discussion of Datasets}\label{sec:discussion-of-datasets}

    \subsection{Dataset 1 \datasetone}\label{subsec:dataset-1datasetone}
    According to the dataset description\cite{Dua:2019} it contains features which attempt to classify a binary label which is whether or not the sample "makes more than \$50k USD" per year.
    This dataset is interesting due to its size ($>$ 40k rows) which allowed me to cover a significant portion of the space made up of the various attributes.
    There are 14 attributes in this dataset, 7 continuous valued attributes and 7 categorical attributes.
    In regards to data distribution with respect to the target attribute, there are approximately 3x as many samples in the category of "less than 50k per year" than in the other category.
    Given the high number of features it seems ripe for dimensionality reduction, especially because some features seem like they would not be correlated with an outcome (like feature "maritial status")

    \subsection{Dataset 2 \datasettwo}\label{subsec:dataset-2datasettwo}
    According to the dataset description this comes from the marketing campaign of a Portuguese bank in the 1990s.\cite{Dua:2019}
    They were intending to sell their customers on a product called "bank term deposit".
    The attributes describe various aspects of each customer including details such as age and employment, as well as that customers financial history with the institution and some other details like previous marketing campaign results and economic indicators such as the euribor 3 month note daily rate and consumer price index.
    This is also a classification problem with the outcome being measured as whether or not they eventually purchased the product in question (binary classification.)
    Given the high number of features it seems ripe for dimensionality reduction, especially because some features seem like they would not be correlated with an outcome (like feature "euribor3m")
    There is an even split between categorical features (10/20) and continuous ones (10/20) in this dataset.
    There are approximately 7x more examples of someone not signing up for this product (as indicated by the "y" value) than those that do.


    \section{Clustering Discussion}\label{sec:clustering-discussion}
    \input{clustering-discussion}


    \section{Dimensionality Reduction Discussion}\label{sec:dimensionality-reduction-discussion}

    \input{dimensionality-reduction.tex}


    \section{Clustering on Dimensionally Reduced Datasets}\label{sec:clustering-on-dimensionally-reduced-datasets}
    For this section I performed 16 experiments to compare each of the datasets, clustering algorithm, and dimensionality
    reduction technique.
    Below is a table of results for the output metrics I chose to compare.
    \begin{center}
        \begin{tabular}{|c| c | c | c |}
            \hline
            & Normalized Mutual Information & Homogeneity Score & Completeness Score \\
            \hline
            \hline
            DS1 + KMEANS + PCA & 0.001                         & 0.002             & 0.001              \\
            \hline
            DS1 + KMEANS + ICA & 0.009                         & 0.126             & 0.001              \\
            \hline
            DS1 + KMEANS + RP  & 0.001                         & 0.001             & 0.001              \\
            \hline
            DS1 + KMEANS + LDA & 0.230                         & 0.370             & 0.167              \\
            \hline
            DS1 + GMM + PCA    & 0.075                         & 0.072             & 0.079              \\
            \hline
            DS1 + GMM + ICA    & 0.093                         & 0.075             & 0.120              \\
            \hline
            DS1 + GMM + RP     & 0.053                         & 0.073             & 0.042              \\
            \hline
            DS1 + GMM + LDA    & 0.238                         & 0.366             & 0.177              \\
            \hline
            DS2 + KMEANS + PCA & 0.189                         & 0.236             & 0.158              \\
            \hline
            DS2 + KMEANS + ICA & 0.096                         & 0.153             & 0.070              \\
            \hline
            DS2 + KMEANS + RP  & 0.070                         & 0.106             & 0.053              \\
            \hline
            DS2 + KMEANS + LDA & 0.260                         & 0.413             & 0.190              \\
            \hline
            DS2 + GMM + PCA    & 0.236                         & 0.346             & 0.179              \\
            \hline
            DS2 + GMM + ICA    & 0.096                         & 0.153             & 0.070              \\
            \hline
            DS2 + GMM + RP     & 0.234                         & 0.345             & 0.178              \\
            \hline
            DS2 + GMM + LDA    & 0.259                         & 0.421             & 0.187              \\
            \hline
        \end{tabular}
    \end{center}
    Comparing these values you can see that for a reasonable tolerance (relative to each output metric) we have roughly
    similar clusters, and in certain cases clusters which are more homogeneous and complete than the original, implying
    that DR was a net positive.
    \begin{figure}
        \centering
        \includegraphics[width=.9\linewidth]{ds1_kmeans_rp.png}
        \caption{DS1 Kmeans RP}\label{Fig:DS1 Kmeans RP}
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[width=.9\linewidth]{ds2_kmeans_rp.png}
        \caption{DS2 Kmeans RP}\label{Fig:DS2 Kmeans RP}
    \end{figure}
    Specifically let's look at~\ref{Fig:DS1 Kmeans RP} and~\ref{Fig:DS2 Kmeans RP} and understand a bit deeper how datasets 1 and 2 compared for Kmeans and randomized projections.
    I chose these (there are many more pictures in the repo) specifically because they showed the largest separation between
    clusters and because it demonstrates just how good a random projection can be.
    Look at the separation at the sillohette coeff (red dotted line) for each one, and then look at the cluster labeling
    for the first and second feature.
    The clusters appear to be (despite not having clear shapes) to be well formed.

    \section{NN Learning on Clustered Datasets}\label{sec:nn-learning-on-clustered-datasets}
    Taking the DR discussion further I also used LDA to construct an \texttt{MLPClassifier} from dimensionally reduced data.

    \begin{figure}
        \centering
        \includegraphics[width=.9\linewidth]{ann1.png}
        \caption{NN Learning Curves on DR Data}\label{Fig:NN Learning Curves on DR Data}
    \end{figure}


    \section{NN Learning on Clustered, Dimensionally Reduced Datasets}\label{sec:nn-learning-on-clustered,-dimensionally-reduced-datasets}
    \begin{figure}
        \centering
        \includegraphics[width=.9\linewidth]{ann1.png}
        \caption{NN Learning Curves on DR, Clustered Data}\label{Fig:NN Learning Curves on DR, Clustered Data}
    \end{figure}

    \bibliography{assignment3}
    \bibliographystyle{plain}
\end{document}